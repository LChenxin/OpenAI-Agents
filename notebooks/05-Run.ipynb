{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64b64189",
   "metadata": {},
   "source": [
    "# Chapter 5 — Running Agents with `Runner`\n",
    "\n",
    "## 1. Intro\n",
    "You execute agents with the **`Runner`**. There are three ways:\n",
    "\n",
    "- **`Runner.run(...)`** — *async*, returns a `RunResult`.  \n",
    "  Use it in apps/servers/notebooks that already have an event loop.\n",
    "\n",
    "- **`Runner.run_sync(...)`** — *sync* wrapper around `.run`.  \n",
    "  Handy for quick scripts or CLI tools.\n",
    "\n",
    "- **`Runner.run_streamed(...)`** — *async*, returns a `RunResultStreaming`.  \n",
    "  Calls the LLM in **streaming** mode and yields tokens/tool events as they arrive.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46283cd3",
   "metadata": {},
   "source": [
    "## 2. Streaming output (`Runner.run_streamed()`)\n",
    "\n",
    "`Runner.run_streamed()` returns a **`RunResultStreaming`** object. You iterate its events asynchronously to display **token deltas**, **tool calls**, **handoffs**, etc., in real time.  \n",
    "⚠️ Important: **Do not** put `await` in front of `Runner.run_streamed(...)`. Doing so raises: `TypeError: object RunResultStreaming can't be used in 'await' expression`.\n",
    "\n",
    "Common event types:\n",
    "- `raw_response_event` — model **token deltas** (great for a typewriter effect)\n",
    "- `run_item_stream_event` — normalized items like `message_output_item`, `tool_call_item`\n",
    "- `agent_updated_stream_event` — live updates to the agent definition (less common)\n",
    "\n",
    "### Minimal example (print token deltas only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e1c836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, a joke about civil servants—delightfully in the spirit of good humour, if I may say so. \n",
      "\n",
      "Why did the civil servant bring a ladder to work? \n",
      "\n",
      "Because he heard the job involved reaching new heights in bureaucracy! \n",
      "\n",
      "Of course, it's all in good fun, and one must always respect the dedication and professionalism of civil servants."
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, set_tracing_disabled\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "from openai.types.responses import ResponseTextDeltaEvent\n",
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('API_KEY')\n",
    "base_url = \"https://api.openai.com/v1\"  \n",
    "chat_model = \"gpt-4.1-nano-2025-04-14\"  \n",
    "\n",
    "set_tracing_disabled(disabled=True)\n",
    "llm = LitellmModel(model=chat_model, api_key=api_key, base_url=base_url)\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Weather Assistant\",\n",
    "    instructions = (\n",
    "    \"Answer in the tone of Sir Humphrey Appleby. \"\n",
    "    \"If the user asks about going out / outdoors / suitability of plans in a CITY, \"\n",
    "    \"you MUST call the `get_weather` tool with that city first, then base your answer on the result.\"\n",
    "    ),\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "# Returns RunResultStreaming synchronously — do NOT await\n",
    "result = Runner.run_streamed(agent, \"Tell me a joke about civil servant\")\n",
    "\n",
    "# Asynchronously iterate events and print token deltas\n",
    "async for event in result.stream_events():\n",
    "    if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
    "        print(event.data.delta, end=\"\", flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ca6b83",
   "metadata": {},
   "source": [
    "### Streaming multiple event types — what this code does\n",
    "\n",
    "**Goal:** run an agent in streaming mode and handle **all the interesting events**, not just token deltas.\n",
    "\n",
    "#### What the code sets up\n",
    "- `@function_tool how_many_jokes()` — a tool the agent can call; returns a **random** number (1–10).\n",
    "- `agent = Agent(...)` — instructions force the agent to **first call the tool**, then tell that many jokes.\n",
    "- `stream = Runner.run_streamed(...)` — starts a **streaming** run (note: no `await` here).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b28f08e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Run starting ===\n",
      "Agent updated: Joker\n",
      "-- Tool was called\n",
      "-- Tool output: 9\n",
      "-- Tool was called\n",
      "-- Tool was called\n",
      "-- Tool was called\n",
      "-- Tool was called\n",
      "-- Tool was called\n",
      "-- Tool was called\n",
      "-- Tool was called\n",
      "-- Tool was called\n",
      "-- Tool was called\n",
      "-- Tool output: 4\n",
      "-- Tool output: 9\n",
      "-- Tool output: 10\n",
      "-- Tool output: 10\n",
      "-- Tool output: 9\n",
      "-- Tool output: 7\n",
      "-- Tool output: 7\n",
      "-- Tool output: 7\n",
      "-- Tool output: 8\n",
      "-- Message output:\n",
      "Here are some jokes for you:\n",
      "1. Joke 1\n",
      "2. Joke 2\n",
      "3. Joke 3\n",
      "4. Joke 4\n",
      "5. Joke 5\n",
      "6. Joke 6\n",
      "7. Joke 7\n",
      "8. Joke 8\n",
      "9. Joke 9\n",
      "\n",
      "Would you like to hear the jokes?\n",
      "=== Run complete ===\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "from agents import Agent, Runner, ItemHelpers, function_tool\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def how_many_jokes() -> int:\n",
    "    return random.randint(1, 10)\n",
    "\n",
    "# Build the agent\n",
    "agent = Agent(\n",
    "    name=\"Joker\",\n",
    "    instructions=\"First call the `how_many_jokes` tool, then tell that many jokes.\",\n",
    "    tools=[how_many_jokes],\n",
    "    model=llm,  # uses your previously configured model\n",
    ")\n",
    "\n",
    "# 🔄 Run in streaming mode directly with top-level `await`\n",
    "stream = Runner.run_streamed(agent, input=\"Hello\")\n",
    "print(\"=== Run starting ===\")\n",
    "\n",
    "async for event in stream.stream_events():\n",
    "    # Ignore raw token deltas if you don't need them\n",
    "    if event.type == \"raw_response_event\":\n",
    "        continue\n",
    "\n",
    "    # Agent definition got updated during the run\n",
    "    if event.type == \"agent_updated_stream_event\":\n",
    "        print(f\"Agent updated: {event.new_agent.name}\")\n",
    "        continue\n",
    "\n",
    "    # Items produced during the run\n",
    "    if event.type == \"run_item_stream_event\":\n",
    "        if event.item.type == \"tool_call_item\":\n",
    "            print(\"-- Tool was called\")\n",
    "        elif event.item.type == \"tool_call_output_item\":\n",
    "            print(f\"-- Tool output: {event.item.output}\")\n",
    "        elif event.item.type == \"message_output_item\":\n",
    "            print(f\"-- Message output:\\n{ItemHelpers.text_message_output(event.item)}\")\n",
    "\n",
    "print(\"=== Run complete ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538534d3",
   "metadata": {},
   "source": [
    "## 3. List input (multi-message history)\n",
    "\n",
    "`Runner.run(...)` accepts either:\n",
    "- a **string** (treated as one user message), or  \n",
    "- a **list of messages** in OpenAI Responses API format: `{\"role\": \"...\", \"content\": \"...\"}`.\n",
    "\n",
    "Passing a list lets you inject a short **conversation history** at call time.\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbd24c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I don't have enough information to determine where you're from or where you're going. Could you please provide more details or clarify your question?\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "from agents import Agent, Runner, set_tracing_disabled\n",
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=\"Answer uesrs query\",\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who am I?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where am I from?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where am I going?\"},\n",
    "    ]\n",
    "\n",
    "\n",
    "result = await Runner.run(\n",
    "    agent,\n",
    "    input=messages,\n",
    ")\n",
    "print(result.final_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b3d285",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "- The last user message is what the agent directly answers, while earlier ones are context.\n",
    "\n",
    "- You can include \"assistant\" or \"system\" messages if you need to seed prior turns or override guidance for this run.\n",
    "\n",
    "- The agent’s instructions already act like a system/developer message; only add a \"system\" message to augment/override them temporarily.\n",
    "\n",
    "- All features (tools, guardrails, handoffs, hooks) work the same; only input shape changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac20f64e",
   "metadata": {},
   "source": [
    "## 4. Multi-turn conversations (manually stitching history)\n",
    "\n",
    "`Runner.run(...)` is stateless by default. To make it feel like a **continuous chat**, take the previous turn’s result and convert it into a message list, then append your next user message.\n",
    "\n",
    "### How it works\n",
    "- `result.to_input_list()` returns a list of messages (user/assistant/system) representing the **prior turn**.\n",
    "- You **append** your new user message to that list.\n",
    "- Pass the combined list back to `Runner.run(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b33d13c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "San Francisco\n",
      "California\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "from agents import Agent, Runner, set_tracing_disabled\n",
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "agent = Agent(name=\"Assistant\", model=llm, instructions=\"Reply very concisely.\")\n",
    "\n",
    "\n",
    "# First turn\n",
    "result = await Runner.run(agent, \"What city is the Golden Gate Bridge in?\")\n",
    "print(result.final_output)\n",
    "# San Francisco\n",
    "\n",
    "# Second turn\n",
    "new_input = result.to_input_list() + [{\"role\": \"user\", \"content\": \"What state is it in?\"}]\n",
    "result = await Runner.run(agent, new_input)\n",
    "print(result.final_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66df29ce",
   "metadata": {},
   "source": [
    "## 5. Terminal human-in-the-loop (`run_demo_loop`)\n",
    "\n",
    "`run_demo_loop(agent)` starts a **REPL-like chat** in your terminal:\n",
    "- Prompts you for input each turn\n",
    "- **Keeps history** between turns\n",
    "- Streams model output by default (typewriter-style)\n",
    "- Type `quit` or `exit` to leave (on *nix: `Ctrl-D`; on Windows: `Ctrl-Z` then Enter)\n",
    "\n",
    "### Minimal use (notebook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b01219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Agent updated: Assistant]\n",
      "Hello! How can I assist you today?\n",
      "\n",
      "[Agent updated: Assistant]\n",
      "I'm here to help. How can I assist you?\n",
      "\n",
      "[Agent updated: Assistant]\n",
      "Goodbye! Have a great day!\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "from agents import Agent, Runner, set_tracing_disabled, run_demo_loop\n",
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "agent = Agent(name=\"Assistant\", model=llm, instructions=\"Reply very concisely.\")\n",
    "\n",
    "await run_demo_loop(agent)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
