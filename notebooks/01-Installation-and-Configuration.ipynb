{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "febf2d46",
   "metadata": {},
   "source": [
    "# Chapter 1 — Installation & Configuration\n",
    "\n",
    "## 1. Quick Intro\n",
    "\n",
    "This chapter summarizes the **OpenAI Agents SDK Quickstart** and how it fits in the agentic ecosystem.\n",
    "\n",
    "## What is it?\n",
    "The **OpenAI Agents SDK** is a lightweight, production-ready toolkit for building agentic apps with a small set of primitives:\n",
    "**Agents**, **Handoffs**, **Tools**, **Sessions**, **Guardrails**, and **Tracing**.  \n",
    "The Python Quickstart covers:\n",
    "- Creating a project & virtual environment  \n",
    "- Installing `openai-agents`  \n",
    "- Setting your `OPENAI_API_KEY`  \n",
    "- Defining your first Agent and adding handoffs  \n",
    "- Running orchestration via `Runner`  \n",
    "- Adding a **guardrail**  \n",
    "- Viewing **execution traces** in the dashboard  \n",
    "\n",
    "## What does it do?\n",
    "- **Hello-world to multi-agent**: start simple, then connect agents using **handoffs**  \n",
    "- **Operational guardrails**: validate inputs/outputs and ensure reliability  \n",
    "- **Observability**: automatic **tracing** to inspect tool calls and handoffs  \n",
    "- **Optional flavors**: includes voice/realtime quickstarts for streaming agents  \n",
    "\n",
    "---\n",
    "\n",
    "## Where it sits vs. other frameworks\n",
    "\n",
    "| Framework | Primary focus | Strengths | Typical use cases |\n",
    "|------------|----------------|------------|------------------|\n",
    "| **OpenAI Agents SDK (Python)** | Minimal primitives for production agents (agents, tools, handoffs, guardrails, sessions, tracing) | Small API surface, built-in **handoffs**, **guardrails**, and **tracing** | Multi-agent workflows with compliance and observability |\n",
    "| **LangChain + LangGraph** | Low-level orchestration of long-running, stateful agents | Fine-grained graph/state control, mature ecosystem | Complex, controllable agents at scale |\n",
    "| **LlamaIndex (Agents)** | Agent patterns integrated with data & indexing stack | Tight RAG/data tooling; easy to connect to vector stores | Knowledge assistants, data-centric agents |\n",
    "| **CrewAI** | Multi-agent collaboration with roles and memory | Lightweight; “crew” abstraction for teamwork | Task automation, multi-role collaboration |\n",
    "| **AutoGen (Microsoft)** | Event-driven multi-agent programming | Rich communication patterns (e.g., GroupChat) | Research and conversational multi-agent workflows |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01661aea",
   "metadata": {},
   "source": [
    "## 2. Getting Started\n",
    "### 2.1 Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2f4055",
   "metadata": {},
   "source": [
    "**First things first** :\n",
    "\n",
    "``` python\n",
    "pip install \"openai-agents[litellm]\"\n",
    "\n",
    "```\n",
    "**What is it?**   \n",
    "The `litellm` extra installs the **LiteLLM** integration for the OpenAI Agents SDK. With it, your agents can call **non-OpenAI models** (Anthropic, Google Gemini, Mistral, Bedrock, Groq, local/Ollama via LiteLLM, etc.) through a unified interface.\n",
    "\n",
    "**When To use it?** \n",
    "- You want to **mix or migrate** across providers (Anthropic/Gemini/Mistral/Bedrock/etc.) without rewriting agent code. \n",
    "- You’re experimenting with **local or alternative backends** (e.g., via LiteLLM/Ollama or Bedrock). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bb751e",
   "metadata": {},
   "source": [
    "### 2.2 API Configuration and Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "426d6244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load env\n",
    "load_dotenv()\n",
    "api_key = os.getenv('API_KEY')\n",
    "\n",
    "base_url = \"https://api.openai.com/v1\"  # We use openai's model here\n",
    "chat_model = \"gpt-4.1-nano-2025-04-14\"   # We will be using cheaper model as im broke AF\n",
    "emb_model = \"text-embedding-3-small\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2128444e",
   "metadata": {},
   "source": [
    "### 2.3 Construct Agent \n",
    "Before diving deeper, let’s make sure everything is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6e88abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I have a wide range of skills, including:\n",
      "\n",
      "- Answering questions and providing explanations on a variety of topics\n",
      "- Assisting with writing, editing, and proofreading text\n",
      "- Generating creative content like stories, poems, and ideas\n",
      "- Providing summaries and explanations of complex material\n",
      "- Helping with language learning and translation\n",
      "- Offering programming help and coding assistance\n",
      "- Engaging in thoughtful conversations and brainstorming\n",
      "- Giving recommendations for books, movies, and other resources\n",
      "\n",
      "Let me know how I can assist you today!\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "\n",
    "response = completion(\n",
    "    model=chat_model, \n",
    "    messages=[{ \"content\": \"What's your skills?\",\"role\": \"user\"}], \n",
    "    api_base=base_url,\n",
    "    api_key=api_key\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0878efb",
   "metadata": {},
   "source": [
    "Here's what we got as output : \n",
    "\n",
    "\"Hello! I have a wide range of skills, including:\n",
    "\n",
    "- Answering questions and providing explanations on a variety of topics\n",
    "- Assisting with writing, editing, and proofreading text\n",
    "- Generating creative content like stories, poems, and ideas\n",
    "- Providing summaries and explanations of complex material\n",
    "- Helping with language learning and translation\n",
    "- Offering programming help and coding assistance\n",
    "- Engaging in thoughtful conversations and brainstorming\n",
    "- Giving recommendations for books, movies, and other resources\n",
    "\n",
    "Let me know how I can assist you today!\"\n",
    "\n",
    "means that our API key and endpoint are set correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f62e0e6",
   "metadata": {},
   "source": [
    "Now we let **OpenAI Agents SDK** handle the conversation logic.   \n",
    "We’ll connect the same `LiteLLM` model to an Agent — add some instructions, and let it do the talking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf65713f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do programmers prefer dark mode?\n",
      "\n",
      "Because light attracts bugs!\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, set_tracing_disabled\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "\n",
    "\n",
    "# 1) (Optional) turn off tracing if you don’t want execution logs collected\n",
    "set_tracing_disabled(disabled=True)\n",
    "\n",
    "# 2) Configure a LiteLLM-backed chat model\n",
    "llm = LitellmModel(model=chat_model, api_key=api_key, base_url=base_url)\n",
    "\n",
    "# 3) Create an Agent and run it\n",
    "agent = Agent(name=\"Assistant\", model=llm, instructions=\"You are a helpful assistant\")\n",
    "\n",
    "\n",
    "\n",
    "# Async (preferred in web/app backends\n",
    "result = await Runner.run(agent, \"Tell me a programmer joke\")  # Use as .ipynb file as jupyter notebook does not support asyncio\n",
    "\n",
    "# Or sync (CLI / quick scripts)\n",
    "#result = Runner.run_sync(agent, \"Tell me a programmer joke\")  \n",
    "\n",
    "\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6653d379",
   "metadata": {},
   "source": [
    "Here we create an agent that **always replies in English** — even if the user speaks another language.  \n",
    "Two key ideas:\n",
    "- **Behavior via `instructions`**: “You only speak English…” gives the agent a strict style rule.\n",
    "- **Determinism via `ModelSettings`**: `temperature=0.1` makes outputs more **focused and consistent**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ddc0c312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a programmer-themed matchmaking joke for you:\n",
      "\n",
      "Why did the programmer go on a blind date?  \n",
      "Because he heard she had a great \"syntax\" and wanted to \"debug\" his love life!\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, ModelSettings\n",
    "\n",
    "\n",
    "english_agent = Agent(\n",
    "    name=\"English agent\",\n",
    "    model=llm, \n",
    "    instructions=\"You only speak English, Even the user is using other laungage\",\n",
    "    model_settings=ModelSettings(temperature=0.1),\n",
    ")\n",
    "\n",
    "result = await Runner.run(english_agent, \"给我讲个程序员相亲的笑话\")\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0774796",
   "metadata": {},
   "source": [
    "### 2.4  Verbose Logging (stdout) + Logger Levels\n",
    "\n",
    "sometimes you just want to **see what the SDK is doing**. This snippet enables verbose logs and lets you control how noisy they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef37adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import enable_verbose_stdout_logging\n",
    "import logging\n",
    "\n",
    "# 1) Print helpful SDK logs to stdout/stderr\n",
    "enable_verbose_stdout_logging()\n",
    "\n",
    "\n",
    "# 2) Get the Agents logger (or use \"openai.agents.tracing\" for tracing-specific logs)\n",
    "logger = logging.getLogger(\"openai.agents\") # or openai.agents.tracing for the Tracing logger\n",
    "\n",
    "\n",
    "# 3) Pick ONE level you want (DEBUG > INFO > WARNING)\n",
    "\n",
    "# To make all logs show up\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# To make info and above show up\n",
    "logger.setLevel(logging.INFO)\n",
    "# To make warning and above show up\n",
    "logger.setLevel(logging.WARNING)\n",
    "# etc\n",
    "\n",
    "# You can customize this as needed, but this will output to `stderr` by default\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66c7f8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing is disabled. Not creating trace Agent workflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracing is disabled. Not creating trace Agent workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing is disabled. Not creating trace Agent workflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracing is disabled. Not creating trace Agent workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting current trace: no-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting current trace: no-op\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting current trace: no-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting current trace: no-op\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x000001533ECBA390>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x000001533ECBA390>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x000001533ECBA390>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x000001533ECBA390>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running agent Assistant (turn 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running agent Assistant (turn 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running agent Assistant (turn 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running agent Assistant (turn 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001533EC2D910>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001533EC2D910>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001533EC2D910>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001533EC2D910>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling LLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling LLM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling LLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling LLM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received model response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received model response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received model response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received model response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting current trace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resetting current trace\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting current trace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resetting current trace\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL: Hi! Here's a fun fact about RAG (Retrieval-Augmented Generation): RAG models combine the strengths of both retrieval-based and generative models by retrieving relevant documents from a large corpus to help generate more accurate and contextually aware responses. This approach allows them to produce more informed and factual outputs!\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(\"openai.agents\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "result = await Runner.run(agent, \"Say hi and tell me one fun fact about RAG.\")\n",
    "print(\"FINAL:\", result.final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3257009e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing is disabled. Not creating trace Agent workflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracing is disabled. Not creating trace Agent workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing is disabled. Not creating trace Agent workflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracing is disabled. Not creating trace Agent workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting current trace: no-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting current trace: no-op\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting current trace: no-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting current trace: no-op\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x000001533ECB8590>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x000001533ECB8590>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x000001533ECB8590>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x000001533ECB8590>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running agent Assistant (turn 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running agent Assistant (turn 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running agent Assistant (turn 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running agent Assistant (turn 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001533EC2D490>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001533EC2D490>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001533EC2D490>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001533EC2D490>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling LLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling LLM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling LLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling LLM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received model response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received model response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received model response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received model response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting current trace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resetting current trace\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting current trace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resetting current trace\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL: A handoff is the process of transferring control or responsibility from one person, team, or system to another.\n"
     ]
    }
   ],
   "source": [
    "trace_logger = logging.getLogger(\"openai.agents.tracing\")\n",
    "trace_logger.setLevel(logging.DEBUG)\n",
    "if not trace_logger.handlers:\n",
    "    trace_logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "result = await Runner.run(agent, \"Explain what a handoff is in one line.\")\n",
    "print(\"FINAL:\", result.final_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
