{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc5884cd",
   "metadata": {},
   "source": [
    "# Chapter 11 — Multi-Agent Orchestration\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "**Orchestration** = how your app coordinates agents: who runs, in what order, and how the next step is chosen. In practice you’ll mix two styles:\n",
    "\n",
    "- **LLM-led orchestration** — let an agent plan, call tools, and hand off to specialists.\n",
    "- **Code-led orchestration** — drive the flow with explicit Python logic (deterministic, cheap, fast).\n",
    "\n",
    "Use both: start LLM-led for open-ended tasks, then “fence” it with code for reliability, cost, and latency.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. LLM-led orchestration\n",
    "\n",
    "An agent with **instructions + tools + handoffs** can plan: search, retrieve files, run code, and delegate to specialists.\n",
    "\n",
    "**Works best for**: fuzzy goals, exploratory research, variable paths.  \n",
    "**Keys to success**:\n",
    "- Tight **instructions** (what tools exist, when/how to use, boundaries).\n",
    "- **Handoffs** to specialists (writer, planner, analyst, reviewer).\n",
    "- **Self-reflection** loops (ask the model to check its own plan/output).\n",
    "- **Observability** (hooks + tracing) and iterative prompt tuning.\n",
    "- **Eval sets** to prevent regressions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f01daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HANDOFF] Triage Agent → History Tutor\n",
      "\n",
      "[ANSWER 1]\n",
      " The capital of France is Paris.\n",
      "[HANDOFF] Triage Agent → Math Tutor\n",
      "\n",
      "[ANSWER 2]\n",
      " Let's solve the equation \\( 2x + 3 = 11 \\) step by step:\n",
      "\n",
      "1. Subtract 3 from both sides to isolate the term with \\( x \\):\n",
      "   \\[\n",
      "   2x + 3 - 3 = 11 - 3\n",
      "   \\]\n",
      "   \\[\n",
      "   2x = 8\n",
      "   \\]\n",
      "\n",
      "2. Divide both sides by 2 to solve for \\( x \\):\n",
      "   \\[\n",
      "   \\frac{2x}{2} = \\frac{8}{2}\n",
      "   \\]\n",
      "   \\[\n",
      "   x = 4\n",
      "   \\]\n",
      "\n",
      "**Final answer:** \\( x = 4 \\).\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, AgentHooks, RunContextWrapper, set_tracing_disabled\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# --- Model setup ---\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "llm = LitellmModel(\n",
    "    model=\"gpt-4.1-nano-2025-04-14\",   \n",
    "    api_key=api_key,\n",
    "    base_url=\"https://api.openai.com/v1\",\n",
    ")\n",
    "\n",
    "set_tracing_disabled(True)  # keep traces off for a minimal demo\n",
    "\n",
    "# --- Hook: accept both positional/keyword forms to avoid signature errors ---\n",
    "class HandoffLog(AgentHooks):\n",
    "    async def on_handoff(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        SDKs may call on_handoff with positional OR keyword args depending on version.\n",
    "        We'll extract safely either way.\n",
    "        Expected names (when kw): context, agent (to), source (from)\n",
    "        \"\"\"\n",
    "        # kw-first\n",
    "        from_agent = kwargs.get(\"source\")\n",
    "        to_agent = kwargs.get(\"agent\")\n",
    "\n",
    "        # fallback to positionals if needed: (self, context, agent, source) OR variants\n",
    "        if to_agent is None and len(args) >= 2:\n",
    "            # args[1] might be context or agent depending on version; try to detect\n",
    "            for a in args:\n",
    "                if isinstance(a, Agent):\n",
    "                    # first Agent we see (other than self) could be 'agent' or 'source'\n",
    "                    if to_agent is None:\n",
    "                        to_agent = a\n",
    "                    elif from_agent is None:\n",
    "                        from_agent = a\n",
    "\n",
    "        if from_agent and to_agent:\n",
    "            print(f\"[HANDOFF] {from_agent.name} → {to_agent.name}\")\n",
    "        else:\n",
    "            print(\"[HANDOFF] (could not parse from/to agent names for this SDK version)\")\n",
    "\n",
    "# --- Specialists ---\n",
    "history_tutor = Agent(\n",
    "    name=\"History Tutor\",\n",
    "    handoff_description=\"Specialist for historical questions.\",\n",
    "    instructions=\"Answer history questions clearly, with key events and context.\",\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "math_tutor = Agent(\n",
    "    name=\"Math Tutor\",\n",
    "    handoff_description=\"Specialist for math questions.\",\n",
    "    instructions=\"Solve the math problem step-by-step, then give a concise final answer.\",\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "# --- Triage (LLM-led orchestration: it decides via handoffs) ---\n",
    "triage = Agent(\n",
    "    name=\"Triage Agent\",\n",
    "    instructions=(\n",
    "        \"Decide which specialist should handle the user's question. \"\n",
    "        \"If it's math, hand off to Math Tutor; if it's history, hand off to History Tutor. \"\n",
    "        \"You must NOT answer yourself—always hand off to the appropriate specialist.\"\n",
    "    ),\n",
    "    handoffs=[history_tutor, math_tutor],\n",
    "    model=llm,\n",
    "    hooks=HandoffLog(),  # prints handoff decisions\n",
    ")\n",
    "\n",
    "# --- Try two queries; triage chooses the path ---\n",
    "res1 = await Runner.run(triage, \"What is the capital of France?\")\n",
    "print(\"\\n[ANSWER 1]\\n\", res1.final_output)\n",
    "\n",
    "res2 = await Runner.run(triage, \"Solve 2x + 3 = 11. What is x?\")\n",
    "print(\"\\n[ANSWER 2]\\n\", res2.final_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e0d24b",
   "metadata": {},
   "source": [
    "## 3. Code-led orchestration\n",
    "\n",
    "LLMs can self-orchestrate, but for **speed, cost, and determinism**, code-led orchestration is rock solid. The mindset: **use the model to *judge*, use code to *decide***.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "def466c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ROUTER(code)] → history\n",
      "[ROUTER(code)] → math\n",
      "[ROUTER(code)] → research\n",
      "[RESEARCH] Quantum computing is a type of computing that uses principles of quantum mechanics to perform calculations. Unlike classical computers, which use bits (0s and 1s), quantum computers use quantum bits or qubits. Qubits can exist in multiple states simultaneously thanks to superposition, and they can be entangled with each other, allowing quantum computers to process complex problems more efficiently than classical computers for certain tasks. Quantum computing has potential applications in cryptography, optimization, drug discovery, and solving complex scientific problems.\n",
      "[HISTORY] Napoleon Bonaparte was a French military and political leader who rose to prominence during the French Revolution and became Emperor of the French from 1804 to 1814, and briefly in 1815 during the Hundred Days. He is known for his extensive military campaigns across Europe, which significantly shaped European history, and for establishing the Napoleonic Code, a foundational legal framework. His reign ended after his defeat at the Battle of Waterloo in 1815, leading to his exile on Saint Helena.\n",
      "[MATH] Let's solve the equation \\( 2x + 3 = 11 \\) step-by-step:\n",
      "\n",
      "1. Subtract 3 from both sides:\n",
      "\\[ 2x + 3 - 3 = 11 - 3 \\]\n",
      "\\[ 2x = 8 \\]\n",
      "\n",
      "2. Divide both sides by 2:\n",
      "\\[ \\frac{2x}{2} = \\frac{8}{2} \\]\n",
      "\\[ x = 4 \\]\n",
      "\n",
      "**Final answer: 4**\n",
      "[REVIEW round 1] PASS\n",
      "\n",
      "[FINAL OUTPUT]\n",
      "Quantum computing leverages quantum mechanics to perform calculations. Unlike classical computers that use bits (0s and 1s), quantum computers use qubits, which can exist in multiple states simultaneously through superposition and can be entangled. This enables quantum computers to solve certain complex problems more efficiently. Potential applications include cryptography, optimization, drug discovery, and scientific research.\n",
      "------------------------------------------------------------\n",
      "[REVIEW round 1] PASS\n",
      "\n",
      "[FINAL OUTPUT]\n",
      "Napoleon Bonaparte was a French military and political leader who rose to prominence during the French Revolution. He became Emperor of France from 1804 to 1814 and briefly in 1815 during the Hundred Days. Known for his extensive European campaigns and the creation of the Napoleonic Code, his reign ended after his defeat at the Battle of Waterloo in 1815, leading to his exile on Saint Helena.\n",
      "------------------------------------------------------------\n",
      "[REVIEW round 1] PASS\n",
      "\n",
      "[FINAL OUTPUT]\n",
      "Solve \\( 2x + 3 = 11 \\):\n",
      "\n",
      "1. Subtract 3 from both sides:\n",
      "\\[ 2x = 8 \\]\n",
      "\n",
      "2. Divide both sides by 2:\n",
      "\\[ x = 4 \\]\n",
      "\n",
      "**Answer: \\( x = 4 \\)**\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os, re, asyncio\n",
    "from dotenv import load_dotenv\n",
    "from agents import Agent, Runner, set_tracing_disabled, ModelSettings\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "\n",
    "# --- 0) Model setup (used only to PRODUCE content, not to decide orchestration) ---\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "llm = LitellmModel(\n",
    "    model=\"gpt-4.1-nano-2025-04-14\",\n",
    "    api_key=api_key,\n",
    "    base_url=\"https://api.openai.com/v1\",\n",
    ")\n",
    "set_tracing_disabled(True)\n",
    "\n",
    "# --- 1) Specialists: content generators only ---\n",
    "research_agent = Agent(\n",
    "    name=\"Research\",\n",
    "    instructions=\"Answer general knowledge questions clearly and concisely.\",\n",
    "    model=llm,\n",
    "    model_settings=ModelSettings(temperature=0.4),\n",
    ")\n",
    "\n",
    "math_agent = Agent(\n",
    "    name=\"Math\",\n",
    "    instructions=\"Solve math problems step-by-step, end with a single final numeric answer.\",\n",
    "    model=llm,\n",
    "    model_settings=ModelSettings(temperature=0.2),\n",
    ")\n",
    "\n",
    "history_agent = Agent(\n",
    "    name=\"History\",\n",
    "    instructions=\"Explain historical facts with key context and dates in 2–3 sentences.\",\n",
    "    model=llm,\n",
    "    model_settings=ModelSettings(temperature=0.2),\n",
    ")\n",
    "\n",
    "editor_agent = Agent(\n",
    "    name=\"Editor\",\n",
    "    instructions=\"Rewrite the text to be clearer and more concise while preserving meaning.\",\n",
    "    model=llm,\n",
    "    model_settings=ModelSettings(temperature=0.3),\n",
    ")\n",
    "\n",
    "reviewer_agent = Agent(\n",
    "    name=\"Reviewer\",\n",
    "    instructions=(\n",
    "        \"Review the text for clarity and correctness. \"\n",
    "        \"Respond ONLY with PASS if it is good, otherwise respond with FAIL and one short reason.\"\n",
    "    ),\n",
    "    model=llm,\n",
    "    model_settings=ModelSettings(temperature=0.0),\n",
    ")\n",
    "\n",
    "# --- 2) PURE-PYTHON ROUTER: code decides which agent to run ---\n",
    "HISTORY_HINTS = {\"who was\", \"when was\", \"in what year\", \"battle of\", \"revolution\", \"dynasty\", \"emperor\", \"king\", \"queen\"}\n",
    "\n",
    "def determine_route(query: str) -> str:\n",
    "    q = query.lower().strip()\n",
    "    # (a) crude math detection: equation-like or mathy keywords\n",
    "    if re.search(r\"[\\d\\)\\]]\\s*([+\\-/*^]|=)\\s*[\\d\\(\\[]\", q) or \"solve\" in q or \"derivative\" in q:\n",
    "        return \"math\"\n",
    "    # (b) crude history detection: keyword hints\n",
    "    if any(h in q for h in HISTORY_HINTS):\n",
    "        return \"history\"\n",
    "    # (c) default to research\n",
    "    return \"research\"\n",
    "\n",
    "# --- 3) CODE-LED PIPELINE RUNNERS ---\n",
    "async def run_code_led_once(query: str):\n",
    "    # Step 1: route in code\n",
    "    route = determine_route(query)\n",
    "    print(f\"[ROUTER(code)] → {route}\")\n",
    "\n",
    "    # Step 2: run the chosen specialist (NO model planning; we call directly)\n",
    "    agent_by_route = {\"math\": math_agent, \"history\": history_agent, \"research\": research_agent}\n",
    "    base = await Runner.run(agent_by_route[route], query)\n",
    "    print(f\"[{route.upper()}] {base.final_output}\")\n",
    "\n",
    "    # Step 3: serial pipeline: edit → review loop (code-enforced stop condition)\n",
    "    edited = await Runner.run(editor_agent, base.final_output)\n",
    "\n",
    "    # Review until PASS or max rounds\n",
    "    text = edited.final_output\n",
    "    for i in range(3):\n",
    "        verdict = await Runner.run(reviewer_agent, text)\n",
    "        v = verdict.final_output.strip().upper()\n",
    "        print(f\"[REVIEW round {i+1}] {v}\")\n",
    "        if v == \"PASS\":\n",
    "            break\n",
    "        # If FAIL, ask the editor to revise based on that single reason\n",
    "        text = (await Runner.run(editor_agent, f\"Revise this to address the issue: {verdict.final_output}\\n\\n{text}\")).final_output\n",
    "\n",
    "    print(\"\\n[FINAL OUTPUT]\\n\" + text + \"\\n\" + \"-\"*60)\n",
    "\n",
    "# --- 4) PARALLEL DEMO (independent queries in parallel; code decides to parallelize) ---\n",
    "async def run_parallel_demo():\n",
    "    queries = [\n",
    "        \"Who was Napoleon?\",               # history\n",
    "        \"Solve 2x + 3 = 11\",               # math\n",
    "        \"What is quantum computing?\",      # research\n",
    "    ]\n",
    "    await asyncio.gather(*(run_code_led_once(q) for q in queries))\n",
    "\n",
    "# --- 5) Try it ---\n",
    "await run_parallel_demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
