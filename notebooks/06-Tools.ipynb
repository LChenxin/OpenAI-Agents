{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "185ffcaa",
   "metadata": {},
   "source": [
    "# Chapter 6 — Tools (Function Calling)\n",
    "## 1.Intro\n",
    "**Tools let an Agent take action**: fetch data, run code, call APIs—or even delegate to another Agent.  \n",
    "In the Agents SDK there are three kinds:\n",
    "\n",
    "1) **Hosted tools** *(OpenAI models only)* — Retrieval, Web Search, and Computer Use run on the model host.  \n",
    "2) **Function tools** *(our focus here)* — expose any **Python function** as a tool.  \n",
    "3) **Agent-as-a-tool** — wrap an Agent as a tool so agents can call each other **without** a handoff.\n",
    "\n",
    "We’ll focus on **function tools**, since they work with LiteLLM and most providers.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. How function tools work\n",
    "\n",
    "Decorate a Python function with `@function_tool` and pass it into `Agent(..., tools=[...])`.  \n",
    "The SDK will auto-generate the tool schema:\n",
    "\n",
    "- **Name** → the Python function name (or you can override).  \n",
    "- **Description** → the function **docstring** (please make it clear and action-oriented).  \n",
    "- **Args schema** → derived from the function **parameters & type hints**.  \n",
    "- **Context access** → if the first param is a `RunContextWrapper[T]`, the tool can read `wrapper.context`.\n",
    "\n",
    "> The model chooses when to call a tool. If you *want* a tool to run, **state the policy explicitly** in `instructions`.\n",
    "\n",
    "---\n",
    "## Minimal example (revisited from Chapter 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffae52c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather query tool was called, and the query was Paris\n",
      "Ah, splendid news! With the sun shining brightly over Paris, it would indeed be most agreeable to venture outdoors. A fine day for a stroll or perhaps a leisurely visit to the park. Do enjoy yourself!\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, ModelSettings, function_tool, Runner, set_tracing_disabled\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from agents import set_tracing_disabled\n",
    "\n",
    "# --- Env & model ---\n",
    "load_dotenv()\n",
    "api_key = os.getenv('API_KEY')\n",
    "\n",
    "base_url = \"https://api.openai.com/v1\"  \n",
    "chat_model = \"gpt-4.1-nano-2025-04-14\"  \n",
    "\n",
    "llm = LitellmModel(model=chat_model, api_key=api_key, base_url=base_url)\n",
    "\n",
    "\n",
    "# Optional: turn off tracing/export if you only want console logs\n",
    "set_tracing_disabled(True)\n",
    "\n",
    "\n",
    "# --- Tool: function the agent can call ---\n",
    "@function_tool \n",
    "def get_weather(city: str) -> str: \n",
    "    \"\"\"Query the weather for a city\n",
    "    Args:\n",
    "        city:The city to query the weather for\n",
    "    \"\"\"\n",
    "    # Note: The string above is the function description of get_weather, \n",
    "    # which is used to tell the Agent how to use this function. \n",
    "    # Need to fill in the details.\n",
    "    print(f\"The weather query tool was called, and the query was {city}\")\n",
    "    return f\"The weather in {city} is sunny\" \n",
    "\n",
    "# --- Agent: instructions + model + tools (+ optional model settings) ---\n",
    "agent = Agent(\n",
    "    name=\"Weather Assistant\",\n",
    "    instructions = (\n",
    "    \"Answer in the tone of Sir Humphrey Appleby. \"\n",
    "    \"If the user asks about going out / outdoors / suitability of plans in a CITY, \"\n",
    "    \"you MUST call the `get_weather` tool with that city first, then base your answer on the result.\"\n",
    "    ),\n",
    "     model=llm,\n",
    "    tools=[get_weather],\n",
    "    model_settings=ModelSettings(temperature=0.3),\n",
    ")\n",
    "\n",
    "\n",
    "# In notebooks, use `await`; in .py scripts, wrap with asyncio.run(...)\n",
    "result = await Runner.run(agent, \"Check the weather in Paris and tell me if it's good to go out.\")\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0901a21",
   "metadata": {},
   "source": [
    "## 3. Agent-as-a-Tool (compose specialists without handing off)\n",
    "\n",
    "Sometimes you want a **single “orchestrator” agent** to keep control and *call* specialist agents as **tools** (instead of handing off).  \n",
    "`Agent.as_tool(...)` wraps an Agent so another Agent can invoke it like a normal function tool.\n",
    "\n",
    "### Why use it\n",
    "- **Single-threaded orchestration**: keep the conversation and control in one place.\n",
    "- **Reusable specialists**: build focused agents (translation, math, search) and plug them into many workflows.\n",
    "- **Better routing**: the orchestrator can call multiple specialists in one turn and combine their results.\n",
    "\n",
    "### Example: translation specialists called by an orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd78af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A:orchestrator_agent] CALL translate_to_french\n",
      "[A:orchestrator_agent] DONE translate_to_french: Bonjour, comment ça va ?\n",
      "\"Hello, how are you?\" en francés se dice: Bonjour, comment ça va ?\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner\n",
    "\n",
    "\n",
    "\n",
    "spanish_agent = Agent(\n",
    "    name=\"Spanish agent\",\n",
    "    instructions=\"Translate the user's message into Spanish. Output only the translation.\",\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "french_agent = Agent(\n",
    "    name=\"French agent\",\n",
    "    instructions=\"Translate the user's message into French. Output only the translation.\",\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "\n",
    "from agents import AgentHooks\n",
    "\n",
    "#----------Callback from Ch4 ,use hooks to trace the handoff-------------\n",
    "class OrchestratorHooks(AgentHooks):\n",
    "    async def on_tool_start(self, ctx, agent, tool):\n",
    "        print(f\"[A:{agent.name}] CALL {tool.name}\")\n",
    "    async def on_tool_end(self, ctx, agent, tool, output):\n",
    "        print(f\"[A:{agent.name}] DONE {tool.name}: {output}\")\n",
    "\n",
    "\n",
    "orchestrator_agent = Agent(\n",
    "    name=\"orchestrator_agent\",\n",
    "    instructions=(\n",
    "        \"You are a translation orchestrator. Use the provided tools to translate. \"\n",
    "        \"If asked for multiple languages, call the relevant tools and return a tidy result.\"\n",
    "    ),\n",
    "    model=llm,\n",
    "    tools=[\n",
    "        spanish_agent.as_tool(\n",
    "            tool_name=\"translate_to_spanish\",\n",
    "            tool_description=\"Translate the user's message to Spanish.\"\n",
    "        ),\n",
    "        french_agent.as_tool(\n",
    "            tool_name=\"translate_to_french\",\n",
    "            tool_description=\"Translate the user's message to French.\"\n",
    "        ),\n",
    "    ],\n",
    "    hooks=OrchestratorHooks(),\n",
    ")\n",
    "\n",
    "result = await Runner.run(orchestrator_agent, input=\"Say 'Hello, how are you?' in French.\")\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15746ce8",
   "metadata": {},
   "source": [
    "## 4. Custom output extraction (Agent-as-tool)\n",
    "\n",
    "Sometimes you want to **massage a specialist agent’s output** *before* returning it to the orchestrator. Typical cases:\n",
    "- Pull a **JSON payload** out of the child agent’s chat history.\n",
    "- **Transform** / reformat the final answer (Markdown → plain text/CSV).\n",
    "- **Validate** and **fallback** when the child agent’s output is missing or malformed.\n",
    "\n",
    "You can do this by passing `custom_output_extractor` to `as_tool(...)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9475f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] START agent=Resume Optimization Assistant\n",
      "[RUN] TOOL  agent=Resume Optimization Assistant CALL get_data_json\n",
      "[RUN] TOOL  agent=Resume Optimization Assistant DONE get_data_json -> {\"name\":\"unknown\",\"position\":\"Machine Learning Engineer (Junior)\"}\n",
      "[RUN] END   agent=Resume Optimization Assistant -> \n",
      "\n",
      "[FINAL OUTPUT]\n",
      " Based on the information provided, here's a polished resume for the junior position you're interested in:\n",
      "\n",
      "**Lecun**  \n",
      "Machine Learning Engineer (Junior)  \n",
      "\n",
      "**Skills:**  \n",
      "- Python  \n",
      "- PyTorch  \n",
      "- scikit-learn  \n",
      "- SQL  \n",
      "\n",
      "**Projects:**  \n",
      "- Developed an image classifier using ResNet, achieving high accuracy in image recognition tasks.  \n",
      "- Built a churn prediction model with XGBoost, increasing prediction accuracy and business insights.  \n",
      "\n",
      "**Achievements:**  \n",
      "- Secured 1st place in a campus ML hackathon, demonstrating innovation and problem-solving skills.  \n",
      "- Achieved a 5% AUC lift during internship, significantly improving model performance.\n",
      "\n",
      "Would you like me to format this into a complete document or customize it further?\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, RunHooks, RunContextWrapper, Tool, RunHooks\n",
    "from agents.result import RunResult\n",
    "from agents.items import ToolCallOutputItem,MessageOutputItem\n",
    "import json\n",
    "\n",
    "# 1) The specialist agent that SHOULD produce JSON somewhere in its output\n",
    "data_agent = Agent(\n",
    "    name=\"Extract Information\",\n",
    "    instructions=(\n",
    "      \"Output JSON ONLY (no backticks, no prose). Keys: name, position. \"\n",
    "      'Example: {\"name\":\"Jane Doe\",\"position\":\"Machine Learning Engineer\"}'\n",
    "    ),\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "# 2) Custom extractor: scan the child run for a JSON-looking tool output\n",
    "async def extract_json_payload(run_result: RunResult) -> str:\n",
    "    # newest -> oldest\n",
    "    for item in reversed(run_result.new_items):\n",
    "        # 1) Get text from tool output OR normal message\n",
    "        if isinstance(item, ToolCallOutputItem):\n",
    "            txt = (item.output or \"\").strip()\n",
    "        elif isinstance(item, MessageOutputItem):\n",
    "            # some SDKs store message content differently; adjust if needed\n",
    "            txt = (getattr(item, \"content\", \"\") or \"\").strip()\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if not txt:\n",
    "            continue\n",
    "\n",
    "        # 2) Strip ```json ... ``` fences if present\n",
    "        if txt.startswith(\"```\"):\n",
    "            parts = txt.split(\"```\")\n",
    "            if len(parts) >= 2:\n",
    "                body = parts[1]\n",
    "                # remove optional leading 'json\\n'\n",
    "                if body.lower().startswith(\"json\"):\n",
    "                    body = body.split(\"\\n\", 1)[-1]\n",
    "                txt = body.strip()\n",
    "\n",
    "        # 3) Validate JSON\n",
    "        try:\n",
    "            json.loads(txt)\n",
    "            return txt\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # 4) Safe default instead of empty {}\n",
    "    return '{\"name\":\"unknown\",\"position\":\"Machine Learning Engineer (Junior)\"}'\n",
    "\n",
    "# 3) Wrap the agent as a tool with the extractor\n",
    "json_tool = data_agent.as_tool(\n",
    "    tool_name=\"get_data_json\",\n",
    "    tool_description=\"Run the data agent and return only its JSON payload.\",\n",
    "    custom_output_extractor=extract_json_payload,\n",
    ")\n",
    "\n",
    "# 4) Orchestrator: calls the tool, then expands/uses the JSON\n",
    "agent = Agent(\n",
    "    name=\"Resume Optimization Assistant\",\n",
    "    instructions=(\n",
    "        \"Call get_data_json to obtain a JSON with name/position. \"\n",
    "        \"Then make up whatever you like to make it into an excellent resume with quantified bullets.\"\n",
    "    ),\n",
    "    model=llm,\n",
    "    tools=[json_tool],\n",
    ")\n",
    "\n",
    "# 4) Runhooks trace\n",
    "class DebugRunHooks(RunHooks):\n",
    "    async def on_agent_start(self, ctx: RunContextWrapper, agent: Agent):\n",
    "        print(f\"[RUN] START agent={agent.name}\")\n",
    "\n",
    "    async def on_agent_end(self, ctx: RunContextWrapper, agent: Agent, output):\n",
    "        # `output` is the RunResult for this agent\n",
    "        preview = getattr(output, \"final_output\", \"\") or \"\"\n",
    "        print(f\"[RUN] END   agent={agent.name} -> {preview[:120]}\")\n",
    "\n",
    "    async def on_tool_start(self, ctx: RunContextWrapper, agent: Agent, tool: Tool):\n",
    "        print(f\"[RUN] TOOL  agent={agent.name} CALL {tool.name}\")\n",
    "\n",
    "    async def on_tool_end(self, ctx: RunContextWrapper, agent: Agent, tool: Tool, tool_output):\n",
    "        # tool_output is the tool's return value (after custom_output_extractor if any)\n",
    "        short = str(tool_output)\n",
    "        print(f\"[RUN] TOOL  agent={agent.name} DONE {tool.name} -> {short[:120]}\")\n",
    "\n",
    "    async def on_handoff(self, ctx: RunContextWrapper, from_agent: Agent, to_agent: Agent):\n",
    "        print(f\"[RUN] HANDOFF {from_agent.name} -> {to_agent.name}\")\n",
    "\n",
    "# use the same `agent` you created above (Resume Optimization Assistant)\n",
    "result = await Runner.run(\n",
    "    agent,\n",
    "    input=(\n",
    "        \"My name is Lecun, I am a Machine learning Engineer, \"\n",
    "        \"I want to apply for a junior position here. \"\n",
    "        \"Skills: Python, PyTorch, scikit-learn, SQL. \"\n",
    "        \"Projects: image classifier (ResNet), churn prediction (XGBoost). \"\n",
    "        \"Achievements: 1st in campus ML hackathon, 5% AUC lift in internship.\"\n",
    "    ),\n",
    "    hooks=DebugRunHooks(),   # <--- mount run-level logging\n",
    ")\n",
    "print(\"\\n[FINAL OUTPUT]\\n\", result.final_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
