{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b83d9ef",
   "metadata": {},
   "source": [
    "# Chapter 2 — Building your first agents \n",
    "\n",
    "## 1. Quick Intro\n",
    "\n",
    "In this chapter we’ll meet three **foundational building blocks** of the OpenAI Agents SDK and wire them together in a tiny demo (adapted from the official example).\n",
    "\n",
    "## What are the primitives?\n",
    "\n",
    "- **Agent**  \n",
    "  A large language model **with a role/instructions and optional tools**.  \n",
    "  Think “specialized workers” (e.g., *Writer*, *Researcher*, *Editor*).\n",
    "\n",
    "- **Handoffs**  \n",
    "  A structured way for one agent to **delegate a sub-task to another agent**.  \n",
    "  This lets you compose multi-step workflows (e.g., Research → Write → Review).\n",
    "\n",
    "- **Guardrails**  \n",
    "  **Validation checks** around what goes **into** (and optionally out of) an agent.  \n",
    "  Use them to enforce constraints (length, JSON schema, safety rules, etc.).\n",
    "\n",
    "## What we’ll build (mini example)\n",
    "\n",
    "We’ll implement a minimal **three-stage flow** that exercises all primitives:\n",
    "\n",
    "1) A **Writer Agent** drafts a short answer.  \n",
    "2) It **handoffs** to an **Editor Agent** for clarity/length polishing.  \n",
    "3) Before finalizing, a **Guardrail** checks the output (e.g., “≤ 120 words”).  \n",
    "4) Return the clean, validated result.\n",
    "\n",
    "You’ll see how each primitive stays small and composable:\n",
    "- Agents encapsulate **instructions + model**.\n",
    "- Handoffs express **who should act next**.\n",
    "- Guardrails keep the pipeline **safe and predictable**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873e1cd5",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7b5850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "\n",
    "# Load env\n",
    "load_dotenv()\n",
    "api_key = os.getenv('API_KEY')\n",
    "\n",
    "base_url = \"https://api.openai.com/v1\"  # We use openai's model here\n",
    "chat_model = \"gpt-4.1-nano-2025-04-14\"   # We will be using cheaper model as im broke AF\n",
    "emb_model = \"text-embedding-3-small\"\n",
    "\n",
    "llm = LitellmModel(model=chat_model, api_key=api_key, base_url=base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2fbf46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import set_tracing_disabled\n",
    "set_tracing_disabled(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05a41f0",
   "metadata": {},
   "source": [
    "## 2. Define Your First Agent\n",
    "An **Agent** needs at least:\n",
    "- `name` — who this agent is (for logs/tracing & readability)\n",
    "- `instructions` — how it should behave (role, tone, boundaries)\n",
    "\n",
    "You can also attach a **model** per agent. Here we reuse the `llm` you configured earlier (e.g., a LiteLLM-backed provider). Each agent can share the same model **or** use a different one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679b808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent\n",
    "agent = Agent(\n",
    "    name=\"Math Tutor\",\n",
    "    instructions=\"You provide help with math problems. Explain your reasoning at each step and include examples\",\n",
    "    model=llm, # uses the model configured in the previous step\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbbe02b",
   "metadata": {},
   "source": [
    "#### Why this matters\n",
    "\n",
    "- Clear `instructions` = consistent behavior.\n",
    "\n",
    "- `model=llm` lets you override per agent (e.g., Math uses Claude, Editor uses GPT).\n",
    "\n",
    "- Easy to mix providers across agents without changing the rest of your pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadfd106",
   "metadata": {},
   "source": [
    "## 3. Add More Agents (with `handoff_description`)\n",
    "\n",
    "Now let’s add a couple of **specialist agents** and describe **when others should hand off** to them.  \n",
    "`handoff_description` is a short, routing-friendly blurb that helps the system decide **who** should take over a sub-task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e8af34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent\n",
    "\n",
    "history_tutor_agent = Agent(\n",
    "    name=\"History Tutor\",\n",
    "    handoff_description=\"Specialist agent for historical questions\",\n",
    "    instructions=\"You provide assistance with historical queries. Explain important events and context clearly.\",\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "math_tutor_agent = Agent(\n",
    "    name=\"Math Tutor\",\n",
    "    handoff_description=\"Specialist agent for math questions\",\n",
    "    instructions=\"You provide help with math problems. Explain your reasoning at each step and include examples\",\n",
    "    model=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d3b30e",
   "metadata": {},
   "source": [
    "#### Why this helps\n",
    "\n",
    "- Clear `handoff_description` → easier delegation (other agents know who to call).\n",
    " \n",
    "- Each agent can share the same `llm` or use a different model tuned to its domain.\n",
    " \n",
    "- Keep descriptions short and unambiguous (e.g., “math questions,” “historical questions”)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa2b856",
   "metadata": {},
   "source": [
    "## 4. Define Handoffs (the “Project Manager”)\n",
    "\n",
    "**Handoffs** let an agent delegate to others. Here we add a **Triage Agent** that decides *who should handle the user’s question* — like a project manager routing tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "207a1823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a router/triage agent that can hand off to specialists\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"Triage Agent\",\n",
    "    instructions=\"You determine which agent to use based on the user's homework question\",\n",
    "    handoffs=[history_tutor_agent, math_tutor_agent],\n",
    "    model=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2b0f9f",
   "metadata": {},
   "source": [
    "### Run !\n",
    "\n",
    "You can now run the triage → specialist flow.   \n",
    "The triage agent will select the proper tutor and return the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7073d037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris. Paris is not only the largest city in France but also historically significant as a cultural, political, and economic center. It has been an important city throughout history, especially during the Middle Ages and the Renaissance, and is renowned for landmarks such as the Eiffel Tower, Notre-Dame Cathedral, and the Louvre Museum.\n"
     ]
    }
   ],
   "source": [
    "result = await Runner.run(triage_agent, \"What is the capital of France?\")\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2df99a8",
   "metadata": {},
   "source": [
    "As a script (.py)\n",
    "\n",
    "```python\n",
    "from agents import Runner\n",
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    result = await Runner.run(triage_agent, \"What is the capital of France?\")\n",
    "    print(result.final_output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd28116",
   "metadata": {},
   "source": [
    "## 5. Add Guardrails (validate inputs before routing)\n",
    "\n",
    "Guardrails let you **inspect/validate** a request **before** an agent acts.  \n",
    "Here we add an **input guardrail** that checks: “Is this a homework question?” If not, we trigger a **tripwire** and stop the flow.\n",
    "\n",
    "### What the code does\n",
    "- **`HomeworkOutput` (Pydantic)**: structured result from a checker agent (`is_homework`, `reasoning`).\n",
    "- **`guardrail_agent`**: a small classifier-style agent that answers in that schema.\n",
    "- **`homework_guardrail`**: runs the checker agent, then returns `GuardrailFunctionOutput`.\n",
    "  - `tripwire_triggered=True` ⇒ block the request.\n",
    "- **`triage_agent`**: adds `input_guardrails=[InputGuardrail(...)]` so every incoming user query is screened **before** handoffs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cfccae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import GuardrailFunctionOutput, InputGuardrail, Agent, Runner\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class HomeworkOutput(BaseModel):\n",
    "    is_homework: bool\n",
    "    reasoning: str\n",
    "\n",
    "guardrail_agent = Agent(\n",
    "    name=\"Guardrail check\",\n",
    "    instructions=\"Check if the user is asking about homework.\",\n",
    "    output_type=HomeworkOutput,\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "async def homework_guardrail(ctx, agent, input_data):\n",
    "    result = await Runner.run(guardrail_agent, input_data, context=ctx.context)\n",
    "    final_output = result.final_output_as(HomeworkOutput)\n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info=final_output,\n",
    "        tripwire_triggered=not final_output.is_homework,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec023e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "triage_agent = Agent(\n",
    "    name=\"Triage Agent\",\n",
    "    instructions=\"You determine which agent to use based on the user's homework question\",\n",
    "    handoffs=[history_tutor_agent, math_tutor_agent],\n",
    "\n",
    "    input_guardrails=[\n",
    "        InputGuardrail(guardrail_function=homework_guardrail),\n",
    "    ],\n",
    "    model=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb726c4",
   "metadata": {},
   "source": [
    "#### What happens at runtime\n",
    "\n",
    "- If the user asks, e.g., “What is the capital of France?” → `is_homework=True` → triage continues and delegates.\n",
    "\n",
    "- If the user asks, “Does drinking tea good for the body?” → `is_homework=False` → guardrail trips and raisess error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c0903b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = await Runner.run(triage_agent, \"does drinking tee good for the body?\")\n",
    "# print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbac4a93",
   "metadata": {},
   "source": [
    "#### How to handle the error \n",
    "\n",
    "If you prefer a graceful message instead of an exception, catch it and reply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa93df42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This assistant only handles homework questions. Please ask a math or history homework question. 🙏\n"
     ]
    }
   ],
   "source": [
    "from agents.exceptions import InputGuardrailTripwireTriggered\n",
    "\n",
    "try:\n",
    "    result = await Runner.run(triage_agent, \"does drinking tea good for the body?\")\n",
    "    print(result.final_output)\n",
    "except InputGuardrailTripwireTriggered as e:\n",
    "    print(\"This assistant only handles homework questions. Please ask a math or history homework question. 🙏\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64255a3d",
   "metadata": {},
   "source": [
    "Expecept output :\"This assistant only handles homework questions. Please ask a math or history homework question. 🙏\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f273e5",
   "metadata": {},
   "source": [
    "## 5. Complete Example (Agents · Handoffs · Guardrail)\n",
    "\n",
    "Below is the **all-in-one script** you can run as a `.py` file.  \n",
    "It wires up two specialist agents (History, Math), a **triage** agent that delegates via **handoffs**, and an **input guardrail** that only allows *homework* questions.\n",
    "\n",
    "```python\n",
    "from agents import Agent, Runner, set_tracing_disabled, GuardrailFunctionOutput, InputGuardrail\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# --- Config & Model ---\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"mistral_key\")  # set in your .env (e.g., mistral_key=sk-xxxx)\n",
    "base_url = \"https://api.mistral.ai/v1\"\n",
    "chat_model = \"mistral/mistral-small-latest\"\n",
    "\n",
    "set_tracing_disabled(disabled=True)  # optional: disable SDK tracing/export\n",
    "llm = LitellmModel(model=chat_model, api_key=api_key, base_url=base_url)\n",
    "\n",
    "# --- Guardrail schema ---\n",
    "class HomeworkOutput(BaseModel):\n",
    "    is_homework: bool\n",
    "    reasoning: str\n",
    "\n",
    "# --- Specialist agents ---\n",
    "history_tutor_agent = Agent(\n",
    "    name=\"History Tutor\",\n",
    "    handoff_description=\"Specialist agent for historical questions\",\n",
    "    instructions=\"You provide assistance with historical queries. Explain important events and context clearly.\",\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "math_tutor_agent = Agent(\n",
    "    name=\"Math Tutor\",\n",
    "    handoff_description=\"Specialist agent for math questions\",\n",
    "    instructions=\"You provide help with math problems. Explain your reasoning at each step and include examples\",\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "# --- Guardrail checker agent ---\n",
    "guardrail_agent = Agent(\n",
    "    name=\"Guardrail check\",\n",
    "    instructions=\"Check if the user is asking about homework.\",\n",
    "    output_type=HomeworkOutput,\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "async def homework_guardrail(ctx, agent, input_data):\n",
    "    result = await Runner.run(guardrail_agent, input_data, context=ctx.context)\n",
    "    final_output = result.final_output_as(HomeworkOutput)\n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info=final_output,\n",
    "        tripwire_triggered=not final_output.is_homework,\n",
    "    )\n",
    "\n",
    "# --- Triage (router) agent with handoffs + input guardrail ---\n",
    "triage_agent = Agent(\n",
    "    name=\"Triage Agent\",\n",
    "    instructions=\"You determine which agent to use based on the user's homework question\",\n",
    "    handoffs=[history_tutor_agent, math_tutor_agent],\n",
    "    input_guardrails=[InputGuardrail(guardrail_function=homework_guardrail)],\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "# --- Run (as a script) ---\n",
    "async def main():\n",
    "    result = await Runner.run(triage_agent, \"does drinking tee good for the body?\")\n",
    "    print(result.final_output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
