{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fb3d8ae",
   "metadata": {},
   "source": [
    "# Chapter 7 — Model Context Protocol (MCP) \n",
    "## 1.Overview\n",
    "\n",
    "**What is MCP?**  \n",
    "The **Model Context Protocol** is an open, JSON-RPC–based standard that lets AI apps (hosts) connect LLMs to **external tools, data, and environments** via **MCP servers**. Think of it like a **USB-C port for AI**: plug standardized “devices” (tools/resources) into many different apps without bespoke wiring.\n",
    "\n",
    "---\n",
    "\n",
    "### Core roles\n",
    "\n",
    "- **Host**: the user-facing AI app (e.g., an IDE with AI, a chat UI, or your agents app).\n",
    "- **Client**: the host’s component that manages a **1:1 connection** to an MCP server, handling protocol details.\n",
    "- **Server**: an external program that **exposes capabilities** (tools, resources, prompts) over MCP.\n",
    "\n",
    "> One host can connect to many servers at once. One client connects to one server.\n",
    "\n",
    "---\n",
    "\n",
    "### Why MCP (the problem it solves)\n",
    "\n",
    "- Avoids the **M×N integration** tangle (M apps × N tools) by standardizing the interface.  \n",
    "- Promotes **interoperability** and **modularity**: add/compose new servers without changing hosts.\n",
    "\n",
    "---\n",
    "\n",
    "### High-level flow\n",
    "\n",
    "1. **User input** → host receives intent.  \n",
    "2. **Host reasoning** → decides which external capabilities might be needed.  \n",
    "3. **Client connects** → to the relevant server(s).  \n",
    "4. **Capability discovery** → list tools/resources/prompts.  \n",
    "5. **Invoke capability** → host (via client) calls a tool on the server.  \n",
    "6. **Server executes** → returns results.  \n",
    "7. **Integrate results** → host feeds them into the LLM context or renders to the user.\n",
    "\n",
    "---\n",
    "\n",
    "### Transport & messages\n",
    "\n",
    "- **JSON-RPC 2.0** for message format:\n",
    "  - **Request** (id, method, params)\n",
    "  - **Response** (result or error)\n",
    "  - **Notification** (one-way event)\n",
    "- **Transports**:\n",
    "  - **stdio** (local child process; simple, sandboxed by OS)\n",
    "  - **HTTP + SSE / streamed HTTP** (remote/cloud; works across networks, server-push updates)\n",
    "\n",
    "---\n",
    "\n",
    "### Using MCP with the OpenAI Agents SDK (conceptual)\n",
    "\n",
    "The Agents SDK can consume MCP-exposed tools through an MCP client so your **agent can call them like any other tool**. Two common patterns:\n",
    "\n",
    "1) **Direct use as tools** — discover MCP tools and register them on your agent.  \n",
    "2) **Orchestrator agent** — your agent routes among **local function tools**, **agent-as-tool** specialists, and **MCP tools** from external servers in one workflow.\n",
    "\n",
    "> Exact import paths / helpers can vary by SDK version. The idea: **connect a client, discover tools, adapt them into the agent’s `tools=[...]` list**, and let the model call them under your instructions/guardrails.\n",
    "\n",
    "---\n",
    "\n",
    "### How this compares\n",
    "\n",
    "| Capability type      | Where it runs            | How you define it                  | Best for |\n",
    "|---|---|---|---|\n",
    "| **Function tool**    | Your app process         | Python function + `@function_tool` | In-process logic/APIs you own |\n",
    "| **Agent-as-tool**    | Your app process         | `other_agent.as_tool(...)`         | Composing specialists w/o handoff |\n",
    "| **MCP tool**         | External MCP server      | MCP server exposes tools           | Reusing external capabilities across apps |\n",
    "\n",
    "---\n",
    "\n",
    "### Quick quiz (recap)\n",
    "\n",
    "- **Main purpose of MCP?** → **Enable LLMs to connect to external data/tools** via a standard (interoperability).  \n",
    "- **What problem does MCP target?** → The **M×N integration** explosion.  \n",
    "- **Key advantage?** → **Standardization & interoperability** across the AI ecosystem.  \n",
    "- **“Host” means…?** → The **user-facing** AI application.  \n",
    "- **JSON-RPC in MCP?** → It’s the **message format** used between client and server.  \n",
    "- **MCP SDK’s role?** → Makes it **easier to implement** clients/servers (handles JSON-RPC ser/de, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "### Practical tips\n",
    "\n",
    "- Start with a **local stdio server** during development (easy to run, easy to debug).  \n",
    "- Keep **instructions** explicit (“Use the MCP tool X when Y happens”) so the model reliably calls it.  \n",
    "- Use **hooks/streaming** to observe tool discovery/calls and log results in real time.  \n",
    "- Combine MCP tools with **function tools** and **agent-as-tool** to build rich, modular workflows.\n",
    "\n",
    "> TL;DR: MCP is a **universal connector** for AI apps. With the Agents SDK, you can treat MCP servers as first-class tool providers—discover capabilities once, reuse everywhere, and keep your orchestration clean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a6a49e",
   "metadata": {},
   "source": [
    "# 2.MCP: Code Walkthrough (Server + Client)\n",
    "\n",
    "This section shows a **minimal end-to-end MCP setup**: an MCP **server** that exposes a tiny tool, and an Agents SDK **client** that connects over **SSE** and calls that tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fccf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: 明天从广州到杭州可以买哪些火车票？\n",
      "以下是明天（2025年10月17日）从广州到杭州的可购火车票信息：\n",
      " \n",
      "1. K512（广州白云 -> 杭州南）：出发07:52，抵达04:24，硬座剩余10张票，价189.5元，无座票也有。\n",
      "2. K528（广州白云 -> 杭州南）：出发08:18，抵达01:49，硬座有票，价173.5元，无座也有票。\n",
      "3. G818（广州南 -> 杭州东）：出发08:27，抵达14:16，商务座无票，一等座无票，二等座无票。\n",
      "4. G1184（广州南 -> 杭州西）：出发08:43，抵达17:28，商务座无票，一等座无票，二等座无票。\n",
      "5. G3068（广州东 -> 杭州西）：出发09:09，抵达15:58，商务座剩余6张，价2602元，一等座剩余1张，价1257元，无座有票。\n",
      "6. G3068（广州新塘 -> 杭州西）：出发09:36，抵达15:58，商务座剩余6张，价2520元，一等座剩余1张，价1220元，无座有票。\n",
      "7. D3121（广州东 -> 杭州东）：出发10:43，抵达22:38，一等座有票，价1031元，无座也有。\n",
      "8. G3072（广州东 -> 杭州西）：出发11:36，抵达20:24，二等座有票，价758元，其他座位剩余情况不同。\n",
      "9. G3072（广州新塘 -> 杭州西）：出发12:03，抵达20:24，二等座有票，价727元。\n",
      "10. G800（广州东 -> 杭州东）：出发12:12，抵达18:36，二等座有票，价820元。\n",
      "11. 其他车次也有不同座席剩余情况。\n",
      "\n",
      "请告诉我是否需要帮你预订或查找特定车型的票。\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "from agents import Agent, Runner, set_tracing_disabled\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "from agents.mcp.server import MCPServerSse\n",
    "from agents.model_settings import ModelSettings\n",
    "import os\n",
    "\n",
    "# --- Env & model ---\n",
    "load_dotenv()\n",
    "api_key = os.getenv('API_KEY')\n",
    "\n",
    "base_url = \"https://api.openai.com/v1\"  \n",
    "chat_model = \"gpt-4.1-nano-2025-04-14\"  \n",
    "set_tracing_disabled(disabled=True)\n",
    "llm = LitellmModel(model=chat_model, api_key=api_key, base_url=base_url)\n",
    "\n",
    "\n",
    "async with MCPServerSse(\n",
    "    params={\n",
    "        \"url\": \"xx\",\n",
    "    }\n",
    ") as my_mcp_server: # 建议用这种上下文写法，否则需要手动连接和关闭MCP服务。\n",
    "    agent = Agent(\n",
    "        name=\"Assistant\",\n",
    "        instructions=\"你是一个火车票查询助手，能够查询火车票信息。\",\n",
    "        mcp_servers=[my_mcp_server],\n",
    "        model_settings=ModelSettings(tool_choice=\"required\"),\n",
    "         model=llm,\n",
    "    )\n",
    "\n",
    "    message = \"明天从广州到杭州可以买哪些火车票？\"\n",
    "    print(f\"Running: {message}\")\n",
    "    result = await Runner.run(starting_agent=agent, input=message)\n",
    "    print(result.final_output)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
